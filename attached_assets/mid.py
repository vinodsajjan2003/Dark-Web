# -*- coding: utf-8 -*-
"""mid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dESlX2v6MCqv2tyWSznuZomSc7hAWVXP
"""

!pip install transformers pandas numpy torch scikit-learn pymongo seaborn matplotlib beautifulsoup4 requests[socks] datasets

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from pymongo import MongoClient
import requests
from bs4 import BeautifulSoup

# Load Excel file from Google Drive
df = pd.read_excel('/content/drive/MyDrive/major_project/short.xlsx')

df.head()

from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')

candidate_labels = ["Drugs", "Fraud", "Scam", "Carding", "Exploit", "Malware", "Phishing", "Hacking Services"]

label_embeddings = model.encode(candidate_labels, convert_to_tensor=True)

def label_threat(content):
    content_embedding = model.encode(str(content), convert_to_tensor=True)
    cos_scores = util.cos_sim(content_embedding, label_embeddings)
    best_label_idx = cos_scores.argmax()
    return candidate_labels[best_label_idx]

df.head()


from tqdm import tqdm
#tqdm.pandas() # Import the pandas integration

# Apply tqdm to the length of the DataFrame for progress tracking
for i in tqdm(range(len(df))):
    df.loc[i, 'Threat_Category'] = label_threat(df.loc[i, 'Post Content'])


sns.countplot(data=df, x='Threat_Category')
plt.xticks(rotation=45)
plt.show()

from datasets import Dataset as HFDataset
from transformers import DistilBertTokenizerFast

# Use Fast Tokenizer (Highly Recommended for speed)
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

# Map Threat_Category to Numerical Labels
labels = df['Threat_Category'].unique().tolist()
label2id = {label: idx for idx, label in enumerate(labels)}
id2label = {idx: label for label, idx in label2id.items()}
df['label'] = df['Threat_Category'].map(label2id)

# Ensure 'Post Content' column contains only strings
df['Post Content'] = df['Post Content'].astype(str)

# Now convert to HuggingFace Dataset
dataset = HFDataset.from_pandas(df[['Post Content', 'label']])


from datasets import Dataset as HFDataset, concatenate_datasets
from transformers import DistilBertTokenizerFast
from tqdm.auto import tqdm

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

batch_size = 5000  # Safe for Colab Free RAM
tokenized_batches = []

for i in tqdm(range(0, len(df), batch_size)):
    temp_df = df.iloc[i:i+batch_size][['Post Content', 'label']]
    temp_dataset = HFDataset.from_pandas(temp_df)

    tokenized_dataset = temp_dataset.map(
        lambda x: tokenizer(x['Post Content'], truncation=True, padding='max_length'),
        batched=True,
        remove_columns=['Post Content']
    )

    tokenized_batches.append(tokenized_dataset)
tokenized_dataset = concatenate_datasets(tokenized_batches)

# Merge all tokenized batches
from datasets import concatenate_datasets
tokenized_dataset = concatenate_datasets(tokenized_batches)


save_path = '/content/drive/MyDrive/major_project/tokenized_forum_data'

tokenized_dataset.save_to_disk(save_path)

print(f"Tokenized dataset saved to {save_path}")

# from datasets import load_from_disk

# # Path where tokenized data is stored
# load_path = '/content/drive/MyDrive/major_project/tokenized_forum_data'

# # Load Tokenized Dataset
# final_tokenized_dataset = load_from_disk(load_path)


# Split
split_dataset = tokenized_dataset.train_test_split(test_size=0.2)

train_dataset = split_dataset['train']
test_dataset = split_dataset['test']

from sklearn.metrics import accuracy_score
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(labels))

training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    weight_decay=0.01,
    logging_steps=500,
    save_steps=1000,
    save_total_limit=1,
    fp16=True,
    fp16_full_eval=True,
    logging_dir='./logs',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, p.predictions.argmax(-1))}
)

trainer.train()

model_save_path = '/content/drive/MyDrive/major_project/fine_tuned_distilbert_model'
# Save Fine-tuned Model
trainer.save_model(model_save_path)

# predictions = trainer.predict(test_dataset)
# print("Accuracy:", accuracy_score(test_labels, predictions.predictions.argmax(-1)))
# print(classification_report(test_labels, predictions.predictions.argmax(-1), target_names=labels))


# def predict_threat(text):
#     inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
#     outputs = model(**inputs)
#     prediction = torch.argmax(outputs.logits, dim=1)
#     return id2label[prediction.item()]

# predict_threat("Selling fresh CVV dumps for carding")  # Output: Carding

from sklearn.metrics import accuracy_score, classification_report

# Step 1: Get True Labels from test_dataset
true_labels = test_dataset['label']

# Step 2: Predict using Trainer
predictions = trainer.predict(test_dataset)
predicted_labels = predictions.predictions.argmax(-1)

# Step 3: Evaluation
print("Accuracy:", accuracy_score(true_labels, predicted_labels))

print(classification_report(true_labels, predicted_labels, target_names=labels, zero_division=0))

from sklearn.metrics import accuracy_score, classification_report

# ... (previous code) ...

def predict_threat(text):
    # Use the tokenizer and the fine-tuned model for prediction
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(model.device) # Tokenize and move to the model's device
    outputs = model(**inputs)
    prediction = torch.argmax(outputs.logits, dim=1)
    return id2label[prediction.item()] # Return the predicted label

# ... (rest of the code) ...

print(predict_threat("Selling fresh CVV dumps for carding"))
print(predict_threat("Offering ransomware \as a service"))
print(predict_threat("Trusted vendor of cocaine and heroin"))
print(predict_threat("Get phishing tools for free"))
print(predict_threat("Hiring hackers for DDoS attacks"))
print(predict_threat("Exploit tutorial for SQL Injection"))
print(predict_threat("Paypal scam methods 2024 leaked"))
print(predict_threat("Best dark web malware market"))
print(predict_threat("Drugs available for wholesale prices"))

